0001| Sun Feb  1 11:31:59 2026       
0002| +-----------------------------------------------------------------------------------------+
0003| | NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |
0004| +-----------------------------------------+------------------------+----------------------+
0005| | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
0006| | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
0007| |                                         |                        |               MIG M. |
0008| |=========================================+========================+======================|
0009| |   0  NVIDIA L40S                    On  |   00000000:30:00.0 Off |                    0 |
0010| | N/A   39C    P8             36W /  350W |       0MiB /  46068MiB |      0%      Default |
0011| |                                         |                        |                  N/A |
0012| +-----------------------------------------+------------------------+----------------------+
0013| 
0014| +-----------------------------------------------------------------------------------------+
0015| | Processes:                                                                              |
0016| |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
0017| |        ID   ID                                                               Usage      |
0018| |=========================================================================================|
0019| |  No running processes found                                                             |
0020| +-----------------------------------------------------------------------------------------+
0021| WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
0022| Fetching 104 files:   0%|          | 0/104 [00:00<?, ?it/s]Fetching 104 files:  15%|█▌        | 16/104 [00:00<00:00, 117.88it/s]Fetching 104 files:  41%|████▏     | 43/104 [00:00<00:00, 127.56it/s]Fetching 104 files:  69%|██████▉   | 72/104 [00:00<00:00, 177.81it/s]Fetching 104 files:  89%|████████▉ | 93/104 [00:00<00:00, 174.86it/s]Fetching 104 files: 100%|██████████| 104/104 [00:00<00:00, 159.23it/s]
0023| 11:32:16 [INFO] ============================================================
0024| 11:32:16 [INFO] SEM V5.5 Training - System Info
0025| 11:32:16 [INFO] ============================================================
0026| 11:32:16 [INFO] PyTorch: 2.6.0+cu124
0027| 11:32:16 [INFO] CUDA available: True
0028| 11:32:16 [INFO] CUDA version: 12.4
0029| 11:32:16 [INFO] GPU: NVIDIA L40S
0030| 11:32:16 [INFO] VRAM: 44.4GB
0031| 11:32:16 [INFO] SM count: 142
0032| 11:32:16 [INFO] bf16 supported: True
0033| 11:32:16 [INFO] CPU cores: 8
0034| 11:32:16 [INFO] ============================================================
0035| 11:32:16 [INFO] GPU: NVIDIA L40S (44.4GB)
0036| 11:32:16 [INFO] Device: cuda
0037| 11:32:16 [INFO] Config: configs/a100_optimized.yaml
0038| 11:32:16 [INFO] micro_batch=16, batch=512, accum=32
0039| 11:32:16 [INFO] DRY RUN: 20 steps, real streaming data, full timing
0040| 11:32:16 [INFO] Building SEM V5.5 model...
0041| 11:32:17 [INFO] Parameters: 27,542,120 effective real
0042| 11:32:18 [INFO] torch.compile enabled for CUDA (max-autotune)
0043| 11:32:18 [INFO] AMP enabled with bf16 (native tensor core support)
0044| 11:32:18 [INFO] Gradient checkpointing enabled on 8 Mamba layers
0045| 11:32:18 [INFO] Model build time: 2.1s
0046| 11:32:18 [INFO] SEM V5.5 'Lean Crystal' Training
0047| 11:32:18 [INFO] Device: cuda
0048| 11:32:18 [INFO] Effective batch size: 512 (micro=16 x accum=32)
0049| 11:32:18 [INFO] [TRAIN] Building dataloader...
0050| 11:32:18 [INFO] [DATA] DataLoader created in 0.000s (batch_size=16, workers=4, pin_memory=True)
0051| 11:32:18 [INFO] [TRAIN] Creating data iterator...
0052| 11:32:18 [INFO] [TRAIN] Starting training loop (max_steps=20)...
0053| 11:32:18 [INFO] ============================================================
0054| 11:32:18 [INFO] [TRAIN] Loading first batch...
0055| 11:32:18 [INFO] [PACK] Starting sequence packing (seq_len=2048)...
0056| 11:32:18 [INFO] [PACK] Starting sequence packing (seq_len=2048)...
0057| 11:32:18 [INFO] [PACK] Starting sequence packing (seq_len=2048)...
0058| 11:32:18 [INFO] [PACK] Starting sequence packing (seq_len=2048)...
0059| 11:32:19 [INFO] [DATA] Starting FineWeb-Edu stream from HuggingFaceFW/fineweb-edu
0060| 11:32:19 [INFO] [DATA] Starting FineWeb-Edu stream from HuggingFaceFW/fineweb-edu
0061| 11:32:19 [INFO] [DATA] Split: train, min_score: 2
0062| 11:32:19 [INFO] [DATA] Split: train, min_score: 2
0063| 11:32:19 [INFO] [DATA] Using HF_TOKEN from environment
0064| 11:32:19 [INFO] [DATA] Using HF_TOKEN from environment
0065| 11:32:19 [INFO] [DATA] Starting FineWeb-Edu stream from HuggingFaceFW/fineweb-edu
0066| 11:32:19 [INFO] [DATA] Starting FineWeb-Edu stream from HuggingFaceFW/fineweb-edu
0067| 11:32:19 [INFO] [DATA] Split: train, min_score: 2
0068| 11:32:19 [INFO] [DATA] Split: train, min_score: 2
0069| 11:32:19 [INFO] [DATA] Using HF_TOKEN from environment
0070| 11:32:19 [INFO] [DATA] Using HF_TOKEN from environment
0071| 11:32:20 [INFO] [DATA] Dataset loaded in 0.93s
0072| 11:32:20 [INFO] [DATA] Filtering by min_score >= 2
0073| 11:32:20 [INFO] [DATA] Shuffling with buffer_size=50000
0074| 11:32:20 [INFO] [DATA] Pipeline setup: load=0.929s filter=0.001s shuffle=0.002s
0075| 11:32:20 [INFO] [DATA] Waiting for first document from stream...
0076| 11:32:20 [INFO] [DATA] Dataset loaded in 0.99s
0077| 11:32:20 [INFO] [DATA] Filtering by min_score >= 2
0078| 11:32:20 [INFO] [DATA] Shuffling with buffer_size=50000
0079| 11:32:20 [INFO] [DATA] Pipeline setup: load=0.986s filter=0.001s shuffle=0.001s
0080| 11:32:20 [INFO] [DATA] Waiting for first document from stream...
0081| 11:32:20 [INFO] [DATA] Dataset loaded in 0.91s
0082| 11:32:20 [INFO] [DATA] Filtering by min_score >= 2
0083| 11:32:20 [INFO] [DATA] Shuffling with buffer_size=50000
0084| 11:32:20 [INFO] [DATA] Pipeline setup: load=0.909s filter=0.050s shuffle=0.002s
0085| 11:32:20 [INFO] [DATA] Waiting for first document from stream...
0086| 11:32:20 [INFO] [DATA] Dataset loaded in 1.04s
0087| 11:32:20 [INFO] [DATA] Filtering by min_score >= 2
0088| 11:32:20 [INFO] [DATA] Shuffling with buffer_size=50000
0089| 11:32:20 [INFO] [DATA] Pipeline setup: load=1.037s filter=0.001s shuffle=0.001s
0090| 11:32:20 [INFO] [DATA] Waiting for first document from stream...
0091| 11:32:24 [INFO] [DATA] First doc received (4.2s)
0092| 11:32:25 [INFO] [DATA] First doc received (4.7s)
0093| 11:32:25 [INFO] [TRAIN] First batch loaded successfully!
0094| 11:32:25 [INFO] [DATA] First doc received (4.9s)
0095| 11:32:25 [INFO] [DATA] First doc received (5.1s)
0096| AUTOTUNE mm(32768x64, 64x128)
0097|   triton_mm_29 0.0188 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0098|   triton_mm_36 0.0191 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
0099|   triton_mm_30 0.0193 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
0100|   triton_mm_35 0.0193 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0101|   triton_mm_34 0.0201 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0102|   triton_mm_27 0.0202 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0103|   mm 0.0208 ms 90.3% 
0104|   triton_mm_31 0.0213 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0105|   triton_mm_28 0.0217 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
0106|   triton_mm_26 0.0220 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
0107| SingleProcess AUTOTUNE benchmarking takes 2.4217 seconds and 0.7266 seconds precompiling for 20 choices
0108| E0201 11:32:40.634000 14 site-packages/torch/_inductor/select_algorithm.py:1477] [1/0_1] Exception out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/vb/cvb7zjg6wpu6qwuu7h5xijvb5mhe67mwju7pdz7zhqyjqf26k76o.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4)
0109| W0201 11:32:41.769000 14 site-packages/torch/_inductor/select_algorithm.py:1696] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
0110| AUTOTUNE addmm(32768x64, 32768x256, 256x64)
0111|   triton_mm_6 0.0397 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
0112|   triton_mm_16 0.0405 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0113|   triton_mm_15 0.0405 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
0114|   triton_mm_7 0.0409 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
0115|   triton_mm_11 0.0412 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0116|   triton_mm_12 0.0430 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
0117|   triton_mm_17 0.0436 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
0118|   bias_addmm 0.0444 ms 89.5% 
0119|   triton_mm_13 0.0447 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0120|   triton_mm_9 0.0452 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0121| SingleProcess AUTOTUNE benchmarking takes 2.1424 seconds and 0.0048 seconds precompiling for 20 choices
0122| /opt/conda/lib/python3.11/site-packages/torch/_inductor/lowering.py:1814: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
0123|   warnings.warn(
0124| E0201 11:32:50.871000 14 site-packages/torch/_inductor/select_algorithm.py:1477] [3/0] Exception out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/66/c66ecnlajbxr2wu43m2vxd26fppunwdumhi2mmraradxgfcmvucy.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8)
0125| W0201 11:32:53.048000 14 site-packages/torch/_inductor/select_algorithm.py:1696] [3/0] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
0126| AUTOTUNE mm(32768x128, 128x256)
0127|   triton_mm_48 0.0411 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0128|   triton_mm_49 0.0411 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
0129|   triton_mm_54 0.0419 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0130|   triton_mm_44 0.0425 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
0131|   triton_mm_45 0.0430 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
0132|   triton_mm_47 0.0456 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
0133|   mm 0.0461 ms 89.0% 
0134|   triton_mm_52 0.0463 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
0135|   triton_mm_43 0.0470 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
0136|   triton_mm_53 0.0472 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
0137| SingleProcess AUTOTUNE benchmarking takes 2.1348 seconds and 0.7262 seconds precompiling for 20 choices
