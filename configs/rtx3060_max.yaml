# SEM V5.5 RTX 3060 6GB - Fast Convergence
# Reduced model for fast iteration on 6GB VRAM
# Gradient accumulation for smooth loss curve
# V8 features enabled (sweep-proven combo: Lindblad + Quaternionic + mHC)

model:
  hidden_dim: 256
  num_layers: 4
  vocab_size: 50262
  max_seq_length: 256

training:
  batch_size: 48
  micro_batch_size: 48
  low_vram_mode: true
  gradient_checkpointing: true
  dtype: float32
  learning_rate: 3.0e-4  # SEOP Fix 85: Sqrt-scaled LR for 12K effective batch
  no_amp: true
  no_compile: true
  encoder_lr_scale: 0.016
  weight_decay: 0.1
  warmup_steps: 500  # SEOP Fix 85: Gradual warmup (propagator warmup = 2x = 1000)
  max_steps: 10000
  gradient_clip: 1.0
  unitary_lambda: 0.006
  unitary_clamp_min: 0.01
  unitary_clamp_max: 6.9
  label_smoothing: 0.058
  dataset_name: "HuggingFaceFW/fineweb-edu"
  tokenizer_path: "tokenizer/"
  num_workers: 8
  prefetch_factor: 4
  pin_memory: true
  shuffle_buffer_size: 10000
  scheduler_type: "cosine"
  stable_steps: 0
  decay_steps: 10000
  lr_min_ratio: 0.1
  log_interval: 10
  health_check_interval: 100
  checkpoint_interval: 1000
  keep_checkpoints: 5
  wandb_project: "sem-v8-optimized"
  wandb_enabled: false

v8:
  use_lindblad: true
  use_hybrid_automata: false
  use_quaternionic: true
  use_mhc: true
  mhc_streams: 4
  mhc_num_iters: 5
  mhc_tau: 0.031
  lindblad_gamma: 0.007
  num_lindblad_ops: 2
  curvature_threshold: 0.64
  condition_threshold: 62.0

encoder:
  soft_sparse: true  # SEOP Fix 85: Re-enable soft_sparse with adaptive learnable temp

curriculum:
  enabled: false

distillation:
  enabled: false
