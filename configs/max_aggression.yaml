# SEM V5.5 "Lean Crystal" - Max Aggression Configuration (XPU/NPU Optimized)
# Optimized for: 16GB RAM / 8GB VRAM
# Part of Wave 6: Trainer XPU & RAM Aggression

model:
  hidden_dim: 256          # 256 real units (isomorphic to 128 complex pairs)
  num_layers: 8
  vocab_size: 32768
  max_seq_length: 2048

encoder:
  sdr_sparsity: 8
  sdr_candidates: 32
  sinkhorn_epsilon: 0.1
  sinkhorn_max_iter: 10
  sinkhorn_tol: 1.0e-2

spinor:
  block_size: 8
  num_blocks: 32
  state_dim: 128          # Increased state capacity
  mimo_groups: 16          # Increased parallelism
  d_conv: 4

propagator:
  cayley_dt: 0.005
  cg_max_iter: 20
  cg_tol: 1.0e-4
  lazy_cg: false
  direct_solve: true
  nonlinear_alpha: 0.005
  laplacian_sparsity: 8
  pit_gamma: 1.0

quantizer:

  codebook_size: 512
  group_size: 2
  fisher_ema_decay: 0.999
  outlier_percentile: 0.01
  dead_code_threshold: 50

sampler:
  temperature: 1.0
  top_k: 50
  top_p: 0.95

training:
  # Throughput Aggression
  batch_size: 16
  micro_batch_size: 2
  gradient_checkpointing: false


  
  # Precision
  dtype: float32           # Real-Block Isomorphism (R2) mapping
  
  # Optimization
  learning_rate: 5.0e-5  # Increased from 1e-6 (which was far too conservative)
  weight_decay: 0.05     # Reduced from 0.1 to prevent over-regularization
  warmup_steps: 2000     # Reduced from 8000 for faster convergence start
  max_steps: 100000
  gradient_clip: 5.0     # SEOP Fix 35: raised from 0.5 â€” prevents gradient death after warmup
  unitary_lambda: 0.01   # Increased from 0.005 for stronger unitarity enforcement


  # Data Pipeline (RAM Optimized)
  dataset_name: "HuggingFaceFW/fineweb-edu"
  tokenizer_path: "tokenizer/"
  num_workers: 2
  shuffle_buffer_size: 20000
  
  # Scheduler
  scheduler_type: "wsd"
  stable_steps: 0
  decay_steps: 10000
  lr_min_ratio: 0.05

  # Infrastructure
  log_interval: 5
  health_check_interval: 50
  checkpoint_interval: 1000
  keep_checkpoints: 5
  wandb_project: "sem-v55-max-aggression"
  wandb_enabled: true

curriculum:
  enabled: true
  stages:
    - min_score: 0         # Start with all data to maximize throughput
      seq_len: 1024
      min_steps: 10000

    - min_score: 4
      seq_len: 2048
      min_steps: 40000

distillation:
  enabled: true
  alpha: 0.5
  ema_decay_start: 0.99
  ema_decay_end: 0.9999
  ema_decay_ramp_steps: 10000
  enable_at_stage: 1       # Defer distillation until stage 1 to avoid OOM (teacher doubles memory)
  temperature: 1.5
