curriculum:
  enabled: true
  stages:
  - min_score: 2
    min_steps: 20000
    seq_len: 512
  - min_score: 3
    min_steps: 30000
    seq_len: 512
distillation:
  enabled: false
encoder:
  sdr_candidates: 128
  sdr_sparsity: 32
  sinkhorn_epsilon: 0.05
  sinkhorn_max_iter: 50
  sinkhorn_tol: 0.001
model:
  hidden_dim: 256
  max_seq_length: 512
  num_layers: 8
  vocab_size: 32768
propagator:
  cayley_dt: 0.1
  cg_max_iter: 5
  cg_tol: 1.0e-06
  laplacian_sparsity: 5
  lazy_cg: true
  lazy_cg_tol: 1.0e-06
  nonlinear_alpha: 0.1
quantizer:
  codebook_size: 256
  dead_code_threshold: 100
  fisher_ema_decay: 0.99
  group_size: 2
  outlier_percentile: 0.01
sampler:
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  min_p: 0.0
  typical_p: 1.0
  repetition_penalty: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  no_repeat_ngram_size: 0
  top_a: 0.0
  epsilon_cutoff: 0.0
  eta_cutoff: 0.0
  temperature_last: false
spinor:
  block_size: 8
  d_conv: 4
  mimo_groups: 8
  num_blocks: 32
  state_dim: 64
training:
  batch_size: 32
  checkpoint_interval: 5000
  dataset_name: HuggingFaceFW/fineweb-edu
  decay_steps: 5000
  dtype: float32
  gradient_checkpointing: false  # Disabled - tensor mismatch bug
  gradient_clip: 5.0
  health_check_interval: 100
  keep_checkpoints: 3
  learning_rate: 0.0003
  log_interval: 1
  lr_min_ratio: 0.1
  max_steps: 15
  micro_batch_size: 2
  num_workers: 0
  scheduler_type: wsd
  shuffle_buffer_size: 1000
  stable_steps: 0
  timing_enabled: true
  timing_log_interval: 10
  tokenizer_path: tokenizer/
  wandb_enabled: false
  wandb_project: sem-v55-xpu
  warmup_steps: 2000
  weight_decay: 0.01
