# SEM V5.5 Cloud Training - FAST VERSION for 1-hour target
# Reduces CG iterations and layers for speed

model:
  hidden_dim: 256
  num_layers: 8  # Reduced from 16
  vocab_size: 32768
  max_seq_length: 256  # Reduced from 512

encoder:
  sdr_sparsity: 16
  sdr_candidates: 64
  sinkhorn_epsilon: 0.05
  sinkhorn_max_iter: 20
  sinkhorn_tol: 1.0e-3

spinor:
  block_size: 8
  num_blocks: 32
  state_dim: 128
  mimo_groups: 16
  d_conv: 4

propagator:
  cayley_dt: 0.1
  cg_max_iter: 3  # Reduced from 10
  cg_tol: 1.0e-4
  nonlinear_alpha: 0.1
  laplacian_sparsity: 4

quantizer:
  codebook_size: 512
  group_size: 2
  fisher_ema_decay: 0.999
  outlier_percentile: 0.01
  dead_code_threshold: 50

sampler:
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  min_p: 0.0
  typical_p: 1.0
  repetition_penalty: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  no_repeat_ngram_size: 0
  top_a: 0.0
  epsilon_cutoff: 0.0
  eta_cutoff: 0.0
  temperature_last: false

training:
  batch_size: 512
  micro_batch_size: 16
  gradient_checkpointing: false  # Disabled - tensor mismatch bug
  dtype: float32
  learning_rate: 6.0e-4
  weight_decay: 0.1
  warmup_steps: 100
  max_steps: 3000  # Reduced for 1-hour target
  gradient_clip: 5.0
  dataset_name: "HuggingFaceFW/fineweb-edu"
  tokenizer_path: "tokenizer/"
  num_workers: 2
  shuffle_buffer_size: 5000
  scheduler_type: "wsd"
  stable_steps: 0
  decay_steps: 500
  lr_min_ratio: 0.1
  log_interval: 5  # More frequent logging
  health_check_interval: 100
  checkpoint_interval: 500
  keep_checkpoints: 2
  wandb_project: "sem-v55-cloud"
  wandb_enabled: false

curriculum:
  enabled: false

distillation:
  enabled: false
