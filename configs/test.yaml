# SEOP Fix 58: GPU Saturation - increase batch size to utilize GPU
# GPU was at 14% with batch=4. Increasing to 16 should give 4-8x speedup.
# LR scaled: sqrt(16/4) * 5e-3 = 1e-2
model:
  hidden_dim: 128           # Proven to work (achieved 4.70)
  num_layers: 8
  vocab_size: 50262         # GPT-2 (50257) + 5 special tokens
  max_seq_length: 256

encoder:
  simple_mode: true         # SEOP Fix 52: Direct embedding â†’ complex
  sdr_sparsity: 16
  sdr_candidates: 128
  sinkhorn_epsilon: 0.05
  sinkhorn_max_iter: 20
  sinkhorn_tol: 1.0e-3
  soft_sparse: true
  soft_sparse_temp: 1.0

spinor:
  block_size: 4
  num_blocks: 16
  state_dim: 64
  mimo_groups: 8
  d_conv: 4

propagator:
  cayley_dt: 0.02
  cg_max_iter: 10
  cg_tol: 2.0e-3
  nonlinear_alpha: 0.25
  laplacian_sparsity: 4
  lazy_cg: true
  lazy_cg_tol: 2.0e-3

quantizer:
  codebook_size: 256
  group_size: 2
  fisher_ema_decay: 0.999
  outlier_percentile: 0.01
  dead_code_threshold: 50

sampler:
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  min_p: 0.0
  typical_p: 1.0
  repetition_penalty: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0
  no_repeat_ngram_size: 0
  top_a: 0.0
  epsilon_cutoff: 0.0
  eta_cutoff: 0.0
  temperature_last: false

training:
  # SEOP Fix 58: GPU Saturation settings (batch=8, 16 OOM'd)
  batch_size: 8              # 2x more work per step (was 4)
  micro_batch_size: 8
  gradient_checkpointing: false
  dtype: float32
  learning_rate: 7.0e-3      # Scaled: sqrt(8/4) * 5e-3 = 7e-3
  no_amp: true
  encoder_lr_scale: 1.0
  weight_decay: 0.01
  warmup_steps: 100
  max_steps: 12500           # Same total tokens: 25000*4 / 8 = 12500
  gradient_clip: 3.0
  unitary_lambda: 0.0
  dataset_name: "HuggingFaceFW/fineweb-edu"
  tokenizer_path: "tokenizer/"
  num_workers: 4             # Parallel data loading (was 0)
  shuffle_buffer_size: 10000
  scheduler_type: "cosine"
  stable_steps: 0
  decay_steps: 25000
  lr_min_ratio: 0.1
  label_smoothing: 0.0
  log_interval: 20
  health_check_interval: 50
  checkpoint_interval: 2000
  keep_checkpoints: 3
  wandb_project: "sem-test"
  wandb_enabled: false
  no_compile: true

curriculum:
  enabled: false

distillation:
  enabled: false
